# US Traffic Accident Forecasting ğŸš—ğŸ“Š

Deep learning models for predicting daily traffic accident counts using 2016-2023 US accident data.

![Model Improvements](results/week3_improvements.png)

## ğŸ¯ Key Results

| Model | RÂ² Score | Improvement | Status |
|-------|----------|-------------|--------|
| **GRU (Optimized)** | **0.415** | **+128%** | âœ… Best DL Model |
| LSTM + Attention | 0.297 | +270% | âœ… Strong |
| TCN | 0.201 | +264% | âœ… Good |
| Transformer | 0.027 | +244% | âš ï¸ Needs Work |
| Random Forest (Baseline) | 0.55 | - | ğŸ¯ Target |

**Achievement:** Improved GRU from RÂ² = 0.182 â†’ 0.415 in 2 hours through configuration optimization!

## ğŸ“Š Dataset

### Source
- **Dataset:** [US Accidents (2016-2023) - Kaggle](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents)
- **Original Size:** 7.7M accident records, 49 states (2.9GB)
- **License:** CC-BY-NC-SA-4.0

### Processed Data
- **File:** `data/processed/daily_accidents_features.csv`
- **Size:** 2,568 daily observations
- **Date Range:** 2016-01-14 to 2023-03-31
- **Features:** 23 engineered features including:
  - Temporal: Daily counts, moving averages (7d, 30d), lag features (1, 3, 7, 14, 30 days)
  - Weather: Temperature, humidity, visibility, wind speed, precipitation
  - Derived: Weather Risk Index, standard deviations

### Citation
Moosavi, Sobhan, et al. "A Countrywide Traffic Accident Dataset.", 2019.

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Google Colab (recommended) or Jupyter

### Installation

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/us-traffic-accident-forecasting.git
cd us-traffic-accident-forecasting

# Install dependencies
pip install -r requirements.txt
```

### Download Data

1. Download from [Kaggle](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents)
2. Or use the processed data in `data/processed/`

### Run Notebooks

```bash
# Start Jupyter
jupyter notebook

# Open notebooks in this order:
# 1. notebooks/Week1_EDA.ipynb
# 2. notebooks/Week2_Feature_Engineering.ipynb
# 3. notebooks/Week3_DeepLearning.ipynb
```

## ğŸ”¬ Methodology

### Week 1: Exploratory Data Analysis
- Analyzed 7.7M accident records
- Identified temporal patterns (winter peaks, COVID impact)
- Geographic distribution across 49 states
- Created interactive visualizations

### Week 2: Feature Engineering & Baselines
- **Random Forest:** RÂ² = 0.55, MAE = 662 âœ“
- **Prophet:** RÂ² = 0.14 (struggled with COVID disruption)
- Engineered 23 features (lags, rolling averages, weather index)
- Key finding: Lag_1 (yesterday's count) most predictive (r = 0.82)

### Week 3: Deep Learning Models

**Initial Implementation (7-day forecast, 30-day lookback):**
- GRU: RÂ² = 0.182
- Other models: Negative RÂ² scores

**Optimized Configuration (1-day forecast, 14-day lookback):**

| Model | Architecture | RÂ² | MAE |
|-------|-------------|-----|-----|
| **GRU** | Bidirectional GRU (64 units) | **0.415** | 0.560 |
| LSTM + Attention | BiLSTM (128â†’64) + Attention | 0.297 | 0.636 |
| TCN | 4 dilated conv blocks | 0.201 | 0.650 |
| Transformer | 2 encoder blocks, 4 heads | 0.027 | 0.808 |

**Quick Fixes Applied:**
1. Forecast horizon: 7 days â†’ 1 day (much easier task)
2. Lookback window: 30 days â†’ 14 days (reduced overfitting)
3. Training epochs: 50 â†’ 100 (better convergence)
4. Simplified GRU architecture (85K params vs 200K)

**Results:** GRU reached 75% of RF baseline!

### Week 4: Hyperparameter Optimization (Planned)
- Optuna automated search
- Target: RÂ² > 0.60 (beat baseline!)
- Feature ablation studies

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**
- **Deep Learning:** TensorFlow/Keras
- **ML:** Scikit-learn, Facebook Prophet
- **Data:** Pandas, NumPy, Dask
- **Visualization:** Matplotlib, Seaborn, Plotly
- **Optimization:** Optuna

## ğŸ“ Project Structure

```
us-traffic-accident-forecasting/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ daily_accidents_features.csv    # Engineered features
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Week1_EDA.ipynb                     # Exploratory analysis
â”‚   â”œâ”€â”€ Week2_Feature_Engineering.ipynb     # Baseline models
â”‚   â””â”€â”€ Week3_DeepLearning.ipynb            # Deep learning models
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gru_model.h5                        # Best GRU model
â”‚   â””â”€â”€ tcn_model.h5                        # TCN model
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ model_comparison_results.csv        # Performance metrics
â”‚   â”œâ”€â”€ model_predictions.csv               # Sample predictions
â”‚   â””â”€â”€ week3_improvements.png              # Before/after visualization
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ utils.py                            # Helper functions
â”‚
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ EDA_Report.html                     # Interactive EDA
â”‚   â””â”€â”€ US_Accidents_Map.html               # Geographic visualization
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                               # This file
â””â”€â”€ requirements.txt                        # Dependencies
```

## ğŸ“ˆ Results Summary

### Performance Progress

```
Week 2 Baseline (Random Forest):
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ RÂ² = 0.55

Week 3 Initial (GRU, no optimization):
  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ RÂ² = 0.18

Week 3 Optimized (GRU, quick fixes):
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ RÂ² = 0.42  (75% of baseline!)
```

### Key Findings

âœ… **1-day forecasting** >> multi-day (RÂ² 0.415 vs 0.182)  
âœ… **Lag features** critical (Lag_1 correlation = 0.82)  
âœ… **Model complexity** must match data size  
âœ… **Deep learning viable** when properly configured  

## ğŸ¯ Future Work

### Week 4 (In Progress)
- [ ] Hyperparameter optimization using Optuna
- [ ] Target: RÂ² > 0.60 (beat Random Forest baseline)
- [ ] Feature ablation studies
- [ ] Final technical report

### Long-term
- [ ] Multi-step forecasting (3, 7, 14 days)
- [ ] State-level models
- [ ] Real-time prediction dashboard
- [ ] Weather API integration

## ğŸ” Key Insights

1. **Forecast Horizon Matters Most**
   - Changing 7-day â†’ 1-day: +0.20 RÂ² gain
   - Single biggest improvement factor

2. **Yesterday Predicts Tomorrow**
   - Lag_1 feature: r = 0.82 correlation
   - Traffic patterns are highly autocorrelated

3. **Simpler Can Be Better**
   - Reduced GRU from 200K â†’ 85K parameters
   - Improved RÂ² from 0.18 â†’ 0.42
   - Lesson: Match model to data size

4. **Deep Learning Requires Tuning**
   - Default configs often fail
   - Configuration >> Architecture choice
   - Week 4 optimization will push past baseline

## ğŸ“Š Visualizations

Interactive visualizations available in `visualizations/`:

- **EDA_Report.html:** Comprehensive exploratory analysis
- **US_Accidents_Map.html:** Geographic distribution
- **week3_improvements.png:** Model performance comparison

## ğŸ¤ Contributing

This is an academic project. Feedback and suggestions welcome!

## ğŸ“„ License

MIT License - Free to use for educational purposes

## ğŸ‘¤ Author

**Mario Cuevas**
- Course: Machine Learning Project
- Date: November 2024
- Focus: Time Series Forecasting with Deep Learning

## ğŸ™ Acknowledgments

- Dataset: Moosavi et al. (Kaggle)
- Course instructors and peers
- TensorFlow/Keras community

---

â­ **Star this repo if helpful!**

ğŸ“Š **Status:** Week 3 Complete âœ… | Week 4 In Progress ğŸš§

ğŸ¯ **Goal:** Beat RÂ² = 0.55 baseline with optimized deep learning
